{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70a85a7",
   "metadata": {},
   "source": [
    "# جلسه ۱۵: خلاصه‌سازی، ترجمه و بازنویسی متن Word با هوش مصنوعی"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337764e",
   "metadata": {},
   "source": [
    "\n",
    "🎯 **هدف:**  \n",
    "در این جلسه با کمک هوش مصنوعی (مدل‌های آماده از Transformers) متن یک فایل Word را تحلیل کرده، آن را خلاصه، ترجمه یا بازنویسی می‌کنیم.\n",
    "\n",
    "🛠️ **سرفصل‌ها:**  \n",
    "1. آپلود فایل Word  \n",
    "2. استخراج متن  \n",
    "3. انتخاب نوع پردازش (خلاصه‌سازی، ترجمه، بازنویسی)  \n",
    "4. اجرای پردازش با هوش مصنوعی\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64550b1",
   "metadata": {},
   "source": [
    "## 1. نصب کتابخانه‌های مورد نیاز"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ca704",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420244c",
   "metadata": {},
   "source": [
    "## 2. آپلود فایل Word و استخراج متن"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from docx import Document\n",
    "\n",
    "uploaded = files.upload()\n",
    "text = \"\"\n",
    "\n",
    "for fn in uploaded:\n",
    "    if fn.endswith(\".docx\"):\n",
    "        doc = Document(fn)\n",
    "        text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        print(\"✏️ متن استخراج‌شده:\")\n",
    "        print(text[:500])\n",
    "        break\n",
    "\n",
    "if not text:\n",
    "    print(\"⚠️ فایل Word معتبر یافت نشد.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb0738",
   "metadata": {},
   "source": [
    "## 3. انتخاب عملیات هوشمند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda85bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📌 انتخاب نوع پردازش:\")\n",
    "print(\"1 - خلاصه‌سازی\")\n",
    "print(\"2 - ترجمه به انگلیسی\")\n",
    "print(\"3 - بازنویسی رسمی‌تر\")\n",
    "\n",
    "choice = input(\"شماره عملیات را وارد کنید: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71c51e",
   "metadata": {},
   "source": [
    "## 4. پردازش متن با هوش مصنوعی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee78838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "if choice == \"1\":\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    result = summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    print(\"\\n📄 خلاصه متن:\")\n",
    "    print(result)\n",
    "\n",
    "elif choice == \"2\":\n",
    "    translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fa-en\")\n",
    "    result = translator(text[:1000])[0]['translation_text']\n",
    "    print(\"\\n🌍 ترجمه به انگلیسی:\")\n",
    "    print(result)\n",
    "\n",
    "elif choice == \"3\":\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fa-en\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-fa-en\")\n",
    "    inputs = tokenizer(text[:500], return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "    reformulated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n🪶 بازنویسی رسمی:\")\n",
    "    print(reformulated)\n",
    "\n",
    "else:\n",
    "    print(\"⛔ عملیات نامعتبر.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
